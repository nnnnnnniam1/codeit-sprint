{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c72f21e",
   "metadata": {},
   "source": [
    "# 기계 번역 실습 프로젝트 (한국어 → 영어)\n",
    "\n",
    "## 프로젝트 목표\n",
    "- 한국어 문장을 영어로 번역하는 Seq2Seq 기반 모델 구현\n",
    "- 기본 Seq2Seq 모델과 Attention 적용 모델 총 3가지 모델 구현 및 비교\n",
    "- 데이터 전처리부터 토크나이저, 임베딩, 모델 설계, 학습, 평가까지 전체 파이프라인 완성\n",
    "\n",
    "---\n",
    "\n",
    "## 데이터셋\n",
    "- train_set.json, valid_set.json 사용\n",
    "- JSON 구조: {\"data\": [{\"ko\": 한국어문장, \"mt\": 영어번역문}, ...]}\n",
    "- 대화체, 일상생활 중심 문장으로 구성\n",
    "\n",
    "__\n",
    "\n",
    "## 프로젝트 진행 순서\n",
    "1. 데이터 로드 및 확인\n",
    "2. 텍스트 전처리 및 토큰화\n",
    "3. 어휘 사전 구축 및 인덱스 변환\n",
    "4. 데이터셋 및 DataLoader 구성\n",
    "5. Seq2Seq 모델 (기본) 구현\n",
    "6. Attention 모델 구현\n",
    "7. 모델 학습 및 검증\n",
    "8. 번역 결과 테스트 및 비교 분석\n",
    "9. BLEU 등 정량 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f4a9e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583ed56a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1557f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 환경설정 및 라이브러리 설치\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e66ddb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cpu\n"
     ]
    }
   ],
   "source": [
    "# @title 기본설정\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device : {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd0bf0",
   "metadata": {},
   "source": [
    "# 1. 데이터 로드 및 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9d26819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 파일 경로\n",
    "\n",
    "TRAIN_PATH = 'koen_train_set.json'\n",
    "VALID_PATH = 'koen_valid_set.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2e06749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 문장 수: 1200000\n",
      "검증 문장 수: 150000\n",
      "훈련 샘플 예시:\n",
      "한국어: 원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.\n",
      "영어: If you reply to the color you want, we will start making it right away.\n"
     ]
    }
   ],
   "source": [
    "def load_json_data(path):\n",
    "  with open(path, 'r', encoding='utf-8') as f:\n",
    "    data_json = json.load(f)\n",
    "  data = data_json['data']\n",
    "  # 한국어 문장, 영어 문장 추출\n",
    "  ko_sentences = [item['ko'] for item in data]\n",
    "  en_sentences = [item['mt'] for item in data]\n",
    "  return ko_sentences, en_sentences\n",
    "\n",
    "train_ko, train_en = load_json_data(TRAIN_PATH)\n",
    "valid_ko, valid_en = load_json_data(VALID_PATH)\n",
    "\n",
    "\n",
    "print(f\"훈련 문장 수: {len(train_ko)}\")\n",
    "print(f\"검증 문장 수: {len(valid_ko)}\")\n",
    "\n",
    "print(\"훈련 샘플 예시:\")\n",
    "print(\"한국어:\", train_ko[0])\n",
    "print(\"영어:\", train_en[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb0fdaa",
   "metadata": {},
   "source": [
    "# 2. 텍스트 전처리 및 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7147e5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 예시:\n",
      "['원하시는', '색상을', '회신해', '주시면', '바로', '제작', '들어가겠습니다']\n",
      "['if', 'you', 'reply', 'to', 'the', 'color', 'you', 'want', 'we', 'will', 'start', 'making', 'it', 'right', 'away']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# 띄어쓰기 기준\n",
    "def tokenize(sentence):\n",
    "  # 소문자 변환 및 특수문자 제거\n",
    "  sentence = sentence.lower()\n",
    "  sentence = re.sub(r\"[^a-zA-Z가-힣0-9\\s]\", \"\", sentence)\n",
    "  tokens = sentence.strip().split()\n",
    "  return tokens\n",
    "\n",
    "# 토큰화된 문장 리스트 생성\n",
    "train_ko_tokens = [tokenize(sentence) for sentence in train_ko]\n",
    "train_en_tokens = [tokenize(sentence) for sentence in train_en]\n",
    "\n",
    "print('토큰화 예시:')\n",
    "print(train_ko_tokens[0])\n",
    "print(train_en_tokens[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb808a",
   "metadata": {},
   "source": [
    "# 3. 어휘 사전 구축 및 인덱스 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77df6e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수 토큰 정의\n",
    "PAD_TOKEN = \"<PAD>\" # Padding Token: 문장 길이를 모두 같게 맞출 때 빈자리 트콘\n",
    "SOS_TOKEN = \"<SOS>\" # Start Of Sentence: 문장의 시작을 알리는 토큰\n",
    "EOS_TOKEN = \"<EOS>\" # End Of Sentence: 문장의 끝을 알리는 토큰\n",
    "UNK_TOKEN = \"<UNK>\" # Unknown Token: 어휘 사전에 없는 단어가 등장했을 때 대체하는 토큰\n",
    "\n",
    "special_tokens = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce366681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 단어 집합 크기: 238393\n",
      "영어 단어 집합 크기: 41153\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(tokenized_sentences, min_freq=2):\n",
    "  counter = Counter()\n",
    "  for sentence in tokenized_sentences:\n",
    "    counter.update(sentence)\n",
    "  \n",
    "  # 최소 등장 빈도 이상 단어만 포함\n",
    "  vocab = [ word for word, freq in counter.items() if freq >= min_freq ]\n",
    "  \n",
    "  # 특수 토큰 앞에 추가\n",
    "  vocab = special_tokens + vocab\n",
    "  \n",
    "  # 단어 -> 인덱스 사전\n",
    "  word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "  idx2word = {idx: word for word, idx in word2idx.items()} \n",
    "  \n",
    "  return word2idx, idx2word\n",
    "\n",
    "\n",
    "ko_word2idx, ko_idx2word = build_vocab (train_ko_tokens)\n",
    "en_word2idx, en_idx2word = build_vocab (train_en_tokens)\n",
    "\n",
    "print(f'한국어 단어 집합 크기: {len(ko_word2idx)}')\n",
    "print(f'영어 단어 집합 크기: {len(en_word2idx)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd3b01fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예시:\n",
      "[1, 4, 5, 6, 7, 8, 9, 10, 2]\n",
      "[1, 4, 5, 6, 7, 8, 9, 5, 10, 11, 12, 13, 14, 15, 16, 17, 2]\n"
     ]
    }
   ],
   "source": [
    "# @title 문장 -> 인덱스 시퀀스 변환 (SOS, EOS 추가)\n",
    "\n",
    "def sentence_to_indices(sentence_tokens, word2idx):\n",
    "  indices = [word2idx.get(token, word2idx[UNK_TOKEN]) for token in sentence_tokens]\n",
    "  return [word2idx[SOS_TOKEN]] + indices + [word2idx[EOS_TOKEN]]\n",
    "\n",
    "train_ko_indices = [sentence_to_indices(sentence, ko_word2idx) for sentence in train_ko_tokens]\n",
    "train_en_indices = [sentence_to_indices(sentence, en_word2idx) for sentence in train_en_tokens]\n",
    "\n",
    "print('예시:')\n",
    "print(train_ko_indices[0])\n",
    "print(train_en_indices[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5650b73",
   "metadata": {},
   "source": [
    "# 4. 데이터셋 및 DataLoader 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "053c422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 문장 토큰 평균 길이: 6\n",
      "한국어 문장 토큰 최대 길이: 79\n",
      "한국어 문장 토큰 표준편차: 3\n",
      "추천 MAX_LENGTH: 9\n"
     ]
    }
   ],
   "source": [
    "# 토큰 길이 리스트 만들기\n",
    "ko_lengths = [len(tokenize(sent)) for sent in train_ko]\n",
    "\n",
    "# 평균 길이 계산 (소수점 이하 버림)\n",
    "avg_length = int(np.mean(ko_lengths))\n",
    "max_length = int(np.max(ko_lengths))\n",
    "\n",
    "\n",
    "print(f\"한국어 문장 토큰 평균 길이: {avg_length}\")\n",
    "print(f\"한국어 문장 토큰 최대 길이: {max_length}\")\n",
    "\n",
    "\n",
    "\n",
    "std_length = int(np.std(ko_lengths))\n",
    "print(f\"한국어 문장 토큰 표준편차: {std_length}\")\n",
    "\n",
    "MAX_LENGTH = avg_length + std_length  # 평균 + 표준편차\n",
    "print(f\"추천 MAX_LENGTH: {MAX_LENGTH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be788233",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTh = 30\n",
    "\n",
    "def pad_sequence(seq, max_len, pad_idx):\n",
    "  return seq+[pad_idx] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "  def __init__(self, src_seqs, trg_seqs, src_pad_idx, trg_pad_idx):\n",
    "    self.src_seqs = [pad_sequence(seq, MAX_LENGTH, src_pad_idx) for seq in src_seqs]  \n",
    "    self.trg_seqs = [pad_sequence(seq, MAX_LENGTH, trg_pad_idx) for seq in trg_seqs]\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.src_seqs)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    return torch.tensor(self.src_seqs[idx]), torch.tensor(self.trg_seqs[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52eaa123",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranslationDataset(train_ko_indices, train_en_indices, ko_word2idx[PAD_TOKEN], en_word2idx[PAD_TOKEN])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aabbedf",
   "metadata": {},
   "source": [
    "# 5. Seq2Seq 모델 (기본 GRU) 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7fb9f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, input_dim, embedding_dim, hidden_dim):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "    self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "    \n",
    "  def forward(self, src):\n",
    "    embedded = self.embedding(src)\n",
    "    outputs, hidden = self.gru(embedded)\n",
    "    return outputs, hidden\n",
    "  \n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, output_dim, embedding_dim, hidden_dim):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "    self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "  \n",
    "  def forward(self, input, hidden):\n",
    "    input = input.unsqueeze(1)  # (batch) -> (batch, 1)\n",
    "    embedded = self.embedding(input)\n",
    "    output, hidden = self.gru(embedded)\n",
    "    pred = self.fc(output.squeeze(1))\n",
    "    return pred, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "  def __init__(self, encoder, decoder, device):\n",
    "    super().__init__()\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.device = device\n",
    "    \n",
    "  def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "    batch_size = src.size(0)\n",
    "    trg_len = trg.size(1)\n",
    "    trg_vocab_size = self.decoder.embedding.num_embeddings\n",
    "    \n",
    "    outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(device)\n",
    "    encoder_outputs, hidden = self.encoder(src)\n",
    "    input = trg[:, 0]\n",
    "    \n",
    "    for t in range(1, trg_len):\n",
    "      output, hidden = self.decoder(input, hidden)\n",
    "      outputs[:, t] = output\n",
    "      teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "      top1 = output.argmax(1)\n",
    "      input = trg[:, t] if teacher_force else top1\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a303fa8",
   "metadata": {},
   "source": [
    "# 6. Attention 모델 구현 (Bahdanau Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a644f1",
   "metadata": {},
   "source": [
    "# 7. 모델 학습 및 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0477b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "INPUT_DIM = len(ko_word2idx)\n",
    "OUTPUT_DIM = len(en_word2idx)\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93fc16b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "decoder = Decoder(OUTPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "basic_model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(basic_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=en_word2idx[PAD_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28954ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "  model.train()\n",
    "  epoch_loss = 0\n",
    "  for src, trg in dataloader:\n",
    "    src, trg = src.to(device), trg.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(src, trg)\n",
    "    output_dim = output.shape[-1]\n",
    "    output = output[:, 1:].reshape(-1, output_dim)\n",
    "    trg = trg[:, 1:].reshape(-1)\n",
    "    \n",
    "    loss = criterion(output, trg)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    epoch_loss += loss.item()\n",
    "    \n",
    "  return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727445fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "for epoch in range(10):\n",
    "  loss = train_epoch(basic_model, train_loader, optimizer, criterion, device)\n",
    "  print(f\"[Epoch {epoch+1}] LOSS: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f2d385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a1a9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc40c93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
